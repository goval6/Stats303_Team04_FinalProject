{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33dd6c4c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Project Code\"\n",
    "subtitle: Team name\n",
    "author: Author 1, Author 2, Author 3, and Author 4 \n",
    "date: 12/09/2024\n",
    "number-sections: true\n",
    "abstract: _This file contains the code for the project on <>, as part of the STAT303-1-Sec20&21 course in Fall 2024_.\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    self-contained: true\n",
    "    font-size: 100%\n",
    "    toc-depth: 4\n",
    "    mainfont: serif\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ea9bb",
   "metadata": {},
   "source": [
    "## Data quality check / cleaning / preparation \n",
    "\n",
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.** An example is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a5a4a",
   "metadata": {},
   "source": [
    "### Distribution of variables\n",
    "*By Sylvia Sherwood*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...Plot for distribution of variables...#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b5e83",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "*By Luna Xu*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2755fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import geopandas as gpd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Reading Data\n",
    "project_data = pd.read_csv('My_CHI._My_Future._Programs_20241113.csv')\n",
    "chi_nei=pd.read_csv('CommAreas_20241114.csv')\n",
    "\n",
    "# Exclude Online Program\n",
    "project_data['Geographic Cluster Name'] = project_data.apply(\n",
    "    lambda row: 'online' if row['Meeting Type'] == 'online' and pd.isnull(row['Geographic Cluster Name']) else row['Geographic Cluster Name'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Comparing Geographic Cluster Names and neighborhoodNames in Community Boundaries\n",
    "project_data_unique = project_data['Geographic Cluster Name'].unique()\n",
    "chi_nei_unique = chi_nei['COMMUNITY'].unique()\n",
    "matches = set(project_data_unique).intersection(chi_nei_unique)\n",
    "unmatched_project_data = set(project_data_unique) - matches\n",
    "unmatched_chi_nei = set(chi_nei_unique) - matches\n",
    "print(f\"Matches: {matches}\")\n",
    "print(f\"Unmatched in project_data: {unmatched_project_data}\")\n",
    "print(f\"Unmatched in chi_nei: {unmatched_chi_nei}\")\n",
    "\n",
    "# Extracting Programs with No Geographic Cluster Name or Unstandardized Name\n",
    "unmatched_geocluster_list = list(unmatched_project_data)\n",
    "project_withlatlong=project_data.loc[\n",
    "    ((project_data['Geographic Cluster Name'].isnull()) | (project_data['Geographic Cluster Name'].isin(unmatched_geocluster_list))) & \n",
    "    (project_data['Latitude'].notnull()) & \n",
    "    (project_data['Longitude'].notnull()),\n",
    "    ['Program ID','Latitude','Longitude','Geographic Cluster Name']\n",
    "]\n",
    "\n",
    "# Turning data into shapely format & Mapping \n",
    "project_withlatlong['point_geom']=project_withlatlong.apply(\n",
    "    lambda row: Point(row['Longitude'],row['Latitude']),axis=1\n",
    ")\n",
    "chi_nei['shapely_geom']=chi_nei['the_geom'].apply(wkt.loads)\n",
    "def match_multiploygon(point,multipolygons):\n",
    "    for muultipolygon in multipolygons:\n",
    "        if muultipolygon.contains(point):\n",
    "            return muultipolygon\n",
    "    return None\n",
    "project_withlatlong['shapely_geom']=project_withlatlong['point_geom'].apply(\n",
    "    lambda point: match_multiploygon(point,chi_nei['shapely_geom'])\n",
    ")\n",
    "matched_program_neiname = pd.merge(project_withlatlong,chi_nei,how='left')\n",
    "\n",
    "# Checking Unstandardized Name & the Neighborhood They Mapped to\n",
    "print(matched_program_neiname.groupby('Geographic Cluster Name')['COMMUNITY'].unique())\n",
    "\n",
    "# Result, imputed dataset\n",
    "attempt1_result = matched_program_neiname.loc[matched_program_neiname['COMMUNITY'].notnull(),['Program ID','COMMUNITY']]\n",
    "project_data = pd.merge(project_data,attempt1_result,on='Program ID',how='left')\n",
    "project_data['Neighborhood'] = project_data.apply(\n",
    "    lambda row: row['COMMUNITY'] if pd.notnull(row['COMMUNITY']) else row['Geographic Cluster Name'],\n",
    "    axis=1\n",
    ")\n",
    "project_data.loc[project_data['Neighborhood'].isin(unmatched_geocluster_list),'Neighborhood']= None\n",
    "project_data.loc[project_data['Geographic Cluster Name']=='online',['Neighborhood']]= 'online'\n",
    "project_data=project_data.drop(columns=['COMMUNITY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424479b5",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "*By Luna Xu*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######---------------Creating new variables----------------#########\n",
    "\n",
    "#Creating three socioeconomic status bins\n",
    "chi_ses = pd.read_csv('Census_Data_-_Selected_socioeconomic_indicators_in_Chicago__2008___2012.csv')\n",
    "chi_ses['ses_bin'] = pd.cut(chi_ses['HARDSHIP INDEX'],3,labels=['High-SES','Mid-SES','Low-SES'])\n",
    "chi_ses=chi_ses[chi_ses['COMMUNITY AREA NAME']!='CHICAGO']#drop chicago total measure\n",
    "\n",
    "#Creating Start Year Variable from Start Date \n",
    "project_data['Start Date']=pd.to_datetime(project_data['Start Date'],format='%m/%d/%Y')\n",
    "project_data['Start Year']=project_data['Start Date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11c9b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd74a9",
   "metadata": {},
   "source": [
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc5c7f",
   "metadata": {},
   "source": [
    "### Analysis 1\n",
    "*By \\<Name of person doing the analysis>*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80efcc",
   "metadata": {},
   "source": [
    "### Analysis 2\n",
    "*By Luna Xu*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa5095f",
   "metadata": {},
   "source": [
    "#### Program Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge census dataset with MCMF dataset\n",
    "project_ses=pd.merge(project_data,chi_ses,left_on='Neighborhood',right_on='COMMUNITY AREA NAME',how='left')\n",
    "# filter MCMF in-person programs with neighborhood info\n",
    "project_hasnei_ses=project_ses.loc[project_ses['Neighborhood'].notnull() & (project_ses['Neighborhood']!='online')] \n",
    "# merge census dataset with neighborhood boundary dataset\n",
    "chi_ses['COMMUNITY AREA NAME']=chi_ses['COMMUNITY AREA NAME'].astype(str).str.upper()\n",
    "chi_nei_ses=pd.merge(chi_nei,chi_ses,left_on='COMMUNITY',right_on='COMMUNITY AREA NAME')\n",
    "# create geopandas dataset\n",
    "chi_nei_ses_gdf=gpd.GeoDataFrame(chi_nei_ses,geometry='shapely_geom')\n",
    "# count number of distinct programs by neighborhood\n",
    "program_counts = project_hasnei_ses.groupby('Neighborhood')['Program ID'].nunique().reset_index()\n",
    "program_counts['Program Count']=program_counts['Program ID']\n",
    "program_counts['Program Count']=program_counts['Program Count'].fillna(0)\n",
    "# merge program count with neighborhood geographic & ses information\n",
    "program_count_nei=pd.merge(program_counts,chi_nei_ses_gdf,left_on='Neighborhood',right_on='COMMUNITY')\n",
    "# create geopandas dataset of the previous dataset\n",
    "program_count_nei_gdf=gpd.GeoDataFrame(program_count_nei,geometry='shapely_geom')\n",
    "# slice dataset by ses\n",
    "lowses=program_count_nei_gdf.loc[program_count_nei_gdf['ses_bin']=='Low-SES',:]\n",
    "midses=program_count_nei_gdf.loc[program_count_nei_gdf['ses_bin']=='Mid-SES',:]\n",
    "highses=program_count_nei_gdf.loc[program_count_nei_gdf['ses_bin']=='High-SES',:]\n",
    "#geospatial plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), constrained_layout=True)\n",
    "# Plot for low SES neighborhood\n",
    "lowses.plot(ax=axes[0], column='Program Count', cmap='YlGnBu', \n",
    "            norm=LogNorm(vmin=program_count_nei_gdf['Program Count'].min(), \n",
    "                         vmax=program_count_nei_gdf['Program Count'].max()), \n",
    "            alpha=0.7)\n",
    "for idx, row in lowses.iterrows():\n",
    "    centroid = row['shapely_geom'].centroid\n",
    "    axes[0].annotate(text=int(row['Program Count']),\n",
    "                     xy=(centroid.x, centroid.y),\n",
    "                     fontsize=7, ha='center', color='black', fontweight='bold')\n",
    "axes[0].set_title('Low-SES Neighborhood')\n",
    "# Plot for mid SES neighborhood\n",
    "midses.plot(ax=axes[1], column='Program Count', cmap='YlGnBu', \n",
    "            norm=LogNorm(vmin=program_count_nei_gdf['Program Count'].min(), \n",
    "                         vmax=program_count_nei_gdf['Program Count'].max()), \n",
    "            alpha=0.7)\n",
    "for idx, row in midses.iterrows():\n",
    "    centroid = row['shapely_geom'].centroid\n",
    "    axes[1].annotate(text=int(row['Program Count']),\n",
    "                     xy=(centroid.x, centroid.y),\n",
    "                     fontsize=7, ha='center', color='black', fontweight='bold')\n",
    "axes[1].set_title('Mid-SES Neighborhood')\n",
    "# Plot for high SES neighborhood\n",
    "highses.plot(ax=axes[2], column='Program Count', cmap='YlGnBu', \n",
    "             norm=LogNorm(vmin=program_count_nei_gdf['Program Count'].min(), \n",
    "                          vmax=program_count_nei_gdf['Program Count'].max()), \n",
    "             alpha=0.7)\n",
    "for idx, row in highses.iterrows():\n",
    "    centroid = row['shapely_geom'].centroid\n",
    "    axes[2].annotate(text=int(row['Program Count']),\n",
    "                     xy=(centroid.x, centroid.y),\n",
    "                     fontsize=7, ha='center', color='black', fontweight='bold')\n",
    "axes[2].set_title('High-SES Neighborhood')\n",
    "fig.suptitle('Program Distribution Based On Neighborhood SES', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec2a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge MCMF dataset with neighborhood ses\n",
    "project_by_year_by_ses=pd.merge(project_data,chi_ses,left_on=\"Neighborhood\",right_on=\"COMMUNITY AREA NAME\")\n",
    "# exclude 2025 data\n",
    "project_by_year_by_ses=project_by_year_by_ses[project_by_year_by_ses['Start Year']!=2025]\n",
    "# count distinct programs based on ses & year\n",
    "year_nei_programcount=project_by_year_by_ses.groupby(['Start Year','ses_bin'])['Program ID'].nunique().reset_index()\n",
    "year_nei_programcount=year_nei_programcount.rename(columns={'Program ID':'Program Count'})\n",
    "pivotdf=year_nei_programcount.pivot(index='Start Year',columns='ses_bin',values='Program Count')\n",
    "# create lineplot of program coun by neighborhood ses over time\n",
    "plt.figure(figsize=(10,5))\n",
    "for ses in pivotdf.columns:\n",
    "    plt.plot(pivotdf.index,pivotdf[ses], label=ses)\n",
    "    for year,count in zip(pivotdf.index,pivotdf[ses]):\n",
    "        if year == 2020:\n",
    "            offset = 500 * (pivotdf.columns.tolist().index(ses) + 1)\n",
    "            plt.text(year, count + offset, str(count), fontsize=8, ha='center', va='top')\n",
    "        else:\n",
    "            plt.text(year, count, str(count), fontsize=8, ha='center', va='bottom')\n",
    "plt.title(\"Program Count By Neighborhood SES Over Time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Number Program\")\n",
    "plt.xticks(ticks=pivotdf.index)\n",
    "plt.legend(title=\"Neighborhood SES\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7168ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distinct programs by neighborhood and year\n",
    "pivot_distro_data = (\n",
    "    project_by_year_by_ses\n",
    "    .groupby(['Start Year', 'Neighborhood'])['Program ID']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'Program ID':'Program Count'})\n",
    ")\n",
    "pivot_distro_data = pivot_distro_data.merge(\n",
    "    project_by_year_by_ses[['Start Year', 'Neighborhood', 'ses_bin']].drop_duplicates(),\n",
    "    on=['Start Year', 'Neighborhood'],\n",
    "    how='left'\n",
    ")\n",
    "# create side-by-side boxplot\n",
    "sns.boxplot(pivot_distro_data,x='Start Year',y='Program Count',hue='ses_bin').set_title('A Comparsion of the Distribution of Programs among Neighborhood By SES')\n",
    "# show the top 10 neighborhood with top program count\n",
    "pivot_distro_data.sort_values(by='Program Count',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition a function to create heatmap based on ses and year\n",
    "def generate_heatmap(data, title, label, col_label, row_label, ax):\n",
    "    heatmap_data = data.pivot(index='ses_bin', columns='Start Year', values='Offers').fillna(0)\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".0f\", cmap=\"YlOrBr\", cbar_kws={'label': label}, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(col_label)\n",
    "    ax.set_ylabel(row_label)\n",
    "fig, axes = plt.subplots(1,3, figsize=(14.5, 3.5))\n",
    "# Scholarship Heatmap\n",
    "scholarship_offers_by_neises = (\n",
    "    project_by_year_by_ses[project_by_year_by_ses['Scholarship Available'] == True]\n",
    "    .groupby(['ses_bin', 'Start Year'])\n",
    "    .agg(Offers=('Program ID', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "generate_heatmap(scholarship_offers_by_neises, \"Scholarship Programs by Neighborhood-SES and Year\", \n",
    "                 \"Number of Scholarships Programs\", \"Year\", \"Neighborhood\", axes[0])\n",
    "# Free Food Heatmap\n",
    "freefood_by_neises = (\n",
    "    project_by_year_by_ses[project_by_year_by_ses['Has Free Food'] == True]\n",
    "    .groupby(['ses_bin', 'Start Year'])\n",
    "    .agg(Offers=('Program ID', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "generate_heatmap(freefood_by_neises, \"Free Food Programs by Neighborhood-SES and Year\", \n",
    "                 \"Number of Free Food Programs\", \"Year\", \"Neighborhood\", axes[1])\n",
    "# Paid Programs Heatmap\n",
    "paid_offers_by_neises = (\n",
    "    project_by_year_by_ses[project_by_year_by_ses['Participants Paid'] == 'Paid, Type Unknown']\n",
    "    .groupby(['ses_bin', 'Start Year'])\n",
    "    .agg(Offers=('Program ID', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "# deal with no paid program 2020\n",
    "fillna2020 = pd.DataFrame({\n",
    "    'ses_bin': ['High-SES', 'Mid-SES', 'Low-SES'],\n",
    "    'Start Year': [2020, 2020, 2020],\n",
    "    'Offers': [0, 0, 0]\n",
    "})\n",
    "paid_offers_by_neises = pd.concat([paid_offers_by_neises, fillna2020], ignore_index=True)\n",
    "ses_order = ['High-SES', 'Mid-SES', 'Low-SES']\n",
    "paid_offers_by_neises['ses_bin'] = pd.Categorical(paid_offers_by_neises['ses_bin'], categories=ses_order, ordered=True)\n",
    "paid_offers_by_neises=paid_offers_by_neises.sort_values(by=['ses_bin', 'Start Year']).reset_index(drop=True)\n",
    "generate_heatmap(paid_offers_by_neises, \"Paid Programs by Neighborhood-SES and Year\", \n",
    "                 \"Number of Paid Programs\", \"Year\", \"Neighborhood SES\", axes[2])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f0c35",
   "metadata": {},
   "source": [
    "#### Equity Feature: Scholarship Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group program categories\n",
    "project_by_year_by_ses['Category Group']=project_by_year_by_ses['Category Name'].map(\n",
    "    {\n",
    "       'Work + Career':'Career & Life Skills', \n",
    "       'Managing Money.':'Career & Life Skills', \n",
    "       'Reading & Writing.':'STEM & Writing',\n",
    "       'Music & Art.':'Arts & Humanity', \n",
    "       'Sports + Wellness.':'Sports & Wellbeing', \n",
    "       'Science':'STEM & Writing', \n",
    "       'Food.':'Sports & Wellbeing',\n",
    "       'Academic Support':'STEM & Writing', \n",
    "       'Digital Media.':'Arts & Humanity', \n",
    "       'Performance.':'Arts & Humanity', \n",
    "       'Healthcare':'Sports & Wellbeing',\n",
    "       'Social Studies':'Arts & Humanity', \n",
    "       'Computers.':'STEM & Writing', \n",
    "       'Math':'STEM & Writing', \n",
    "       'Helping Your Community.':'Arts & Humanity',\n",
    "       'Building & Fixing Things':'Career & Life Skills', \n",
    "       'Nature.':'Sports & Wellbeing', \n",
    "       'Teaching':'Career & Life Skills',\n",
    "       'Customer/Human Service':'Career & Life Skills', \n",
    "       'Transportation':'Career & Life Skills', \n",
    "       'Law':'Career & Life Skills'\n",
    "    }\n",
    ")\n",
    "# count distinct programs based on ses and category\n",
    "ses_category_programcount = project_by_year_by_ses[project_by_year_by_ses['Scholarship Available']==True].loc[:,['Program ID','Category Group','ses_bin']].groupby(\n",
    "    ['ses_bin','Category Group']\n",
    "    ).agg(\n",
    "        NumofProgram=('Program ID', 'nunique')\n",
    "        ).reset_index().sort_values(by=['ses_bin', 'NumofProgram'], ascending=[True, False])\n",
    "# side-by-side barplot of scholarship program category distribution by SES\n",
    "sns.barplot(ses_category_programcount,x='ses_bin',y='NumofProgram',hue='Category Group').set_title('Scholarship Program Category Distribution by Neighborhood SES')\n",
    "plt.legend(title='Program Category Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68fb138",
   "metadata": {},
   "source": [
    "#### Equity Feature: Has Free Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count freefood programs by neighborhood\n",
    "freefood_by_nei = (\n",
    "    project_by_year_by_ses[project_by_year_by_ses['Has Free Food'] == True]\n",
    "    .groupby(['Neighborhood'])\n",
    "    .agg(Offers=('Program ID', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "# merge the previous dataset to get hardship index for each neighborhood\n",
    "freefood_by_nei=freefood_by_nei.merge(project_by_year_by_ses[['HARDSHIP INDEX','ses_bin','Neighborhood']].drop_duplicates(),on='Neighborhood')\n",
    "# plot scatter with trendline\n",
    "sns.regplot(freefood_by_nei,x='HARDSHIP INDEX',y='Offers')\n",
    "plt.xlabel('Neighborhood Hardship Index')\n",
    "plt.ylabel('Number of Free Food Programs')\n",
    "plt.title('The Relationship Between Neighborhood Hardship Level and Number of Free Food Programs Offered')\n",
    "# find the top and bottom neighborhood with free food programs\n",
    "freefood_by_nei.sort_values(by='Offers',ascending=False).head(10)\n",
    "freefood_by_nei.sort_values(by='Offers',ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fc235",
   "metadata": {},
   "source": [
    "#### Equity Feature: Participants Paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all paid programs\n",
    "paid_programs=project_by_year_by_ses[project_by_year_by_ses['Participants Paid'] == 'Paid, Type Unknown'].loc[:,['Program ID','Description','Org Name']]\n",
    "paid_programs=paid_programs.set_index('Program ID').drop_duplicates()\n",
    "# download relevant nltk packages\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# create a function that clean and slice descriptions by word\n",
    "def description_cleaner(description):\n",
    "    cleaned_description = re.sub(r'<.*?>', '', description)\n",
    "    words = word_tokenize(cleaned_description.lower())\n",
    "    return [word for word in words if word not in stop_words and word.isalpha()]\n",
    "# create a column that store clean description (list of words)\n",
    "paid_programs['Cleaned_Description']=paid_programs['Description'].apply(description_cleaner)\n",
    "# get all words from all descriptions\n",
    "all_words = [word for description in paid_programs['Cleaned_Description'] for word in description]\n",
    "# count word frequency\n",
    "word_counts = Counter(all_words)\n",
    "# Get the most common words\n",
    "common_words = word_counts.most_common(20) \n",
    "# plot a bar graph to show word frequency\n",
    "word, frequency = zip(*common_words)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(word, frequency, color='skyblue')\n",
    "plt.title('Word Frequencies in Paid Program Description', fontsize=16)\n",
    "plt.xlabel('Words', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)  \n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "# create a function that slice description into sentences\n",
    "def sentence_seperator(description,word):\n",
    "    cleaned_description = re.sub(r'<.*?>', '', description)\n",
    "    sentences = re.split(r'(?<=[.!?])', cleaned_description)\n",
    "    return [sentence for sentence in sentences if word.lower() in sentence.lower()]\n",
    "# find the sentences in descriptions that contain the word \"paid\"\n",
    "paidword_program=project_by_year_by_ses[project_by_year_by_ses['Description'].str.contains('paid', case=False, na=False)]\n",
    "paidword_program=paidword_program.loc[:,['Program ID','Program Name', 'Description','COMMUNITY AREA NAME','Scholarship Available','Participants Paid']]\n",
    "paidword_program=paidword_program.set_index('Program ID').drop_duplicates()\n",
    "not_labeled_paidword_program=paidword_program[(paidword_program['Participants Paid'] != 'Paid, Type Unknown') & (paidword_program['Scholarship Available'] == False)]\n",
    "not_labeled_paidword_program.loc[:,['Matching Sentences']] = not_labeled_paidword_program['Description'].apply(lambda desc: sentence_seperator(desc,'paid'))\n",
    "# display five samples of sentence that contains \"paid\" to inspect whether it actually reflects that the program pays participants\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "not_labeled_paidword_program.loc[:,['Matching Sentences']].sample(5)\n",
    "# find the sentences in descriptions that contain the word \"stipend\"\n",
    "stipend_program=project_by_year_by_ses[project_by_year_by_ses['Description'].str.contains('stipend', case=False, na=False)]\n",
    "stipend_program=stipend_program.loc[:,['Program ID','Program Name', 'Description','COMMUNITY AREA NAME','Scholarship Available','Participants Paid']]\n",
    "stipend_program=stipend_program.set_index('Program ID').drop_duplicates()\n",
    "not_labeled_stipend_program=stipend_program[(stipend_program['Participants Paid'] != 'Paid, Type Unknown') & (stipend_program['Scholarship Available'] == False)]\n",
    "not_labeled_stipend_program.loc[:,['Matching Sentences']] = not_labeled_stipend_program['Description'].apply(lambda desc: sentence_seperator(desc, 'stipend'))\n",
    "# display five samples of sentence that contains \"stipend\" to inspect whether it actually reflects that the program pays participants\n",
    "not_labeled_stipend_program.loc[:,['Matching Sentences']].sample(5)\n",
    "# create a column called stipend and impute it with true if has matching sentence \n",
    "project_by_year_by_ses.loc[\n",
    "    project_by_year_by_ses['Program ID'].isin(not_labeled_stipend_program.index),\n",
    "    'Stipend'\n",
    "] = True\n",
    "# count distinct paid programs based on ses and start year \n",
    "new_paid_offers_by_neises = (\n",
    "    project_by_year_by_ses[(project_by_year_by_ses['Participants Paid'] == 'Paid, Type Unknown') | (project_by_year_by_ses['Stipend']==True)]\n",
    "    .groupby(['ses_bin', 'Start Year'])\n",
    "    .agg(Offers=('Program ID', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "new_paid_offers_by_neises = pd.concat([new_paid_offers_by_neises, fillna2020], ignore_index=True)\n",
    "new_paid_offers_by_neises['ses_bin'] = pd.Categorical(new_paid_offers_by_neises['ses_bin'], categories=ses_order, ordered=True)\n",
    "new_paid_offers_by_neises=new_paid_offers_by_neises.sort_values(by=['ses_bin', 'Start Year']).reset_index(drop=True)\n",
    "# plot two heatmap to compare the result before and after imputing stipend programs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "generate_heatmap(paid_offers_by_neises, \"Paid Programs by Neighborhood-SES and Year\", \n",
    "                 \"Number of Paid Programs\", \"Year\", \"Neighborhood SES\", axes[0])\n",
    "\n",
    "generate_heatmap(new_paid_offers_by_neises, \"Stipend & Paid Programs by Neighborhood-SES and Year\", \n",
    "                 \"Number of Paid Programs\", \"Year\", \"Neighborhood SES\", axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2266f4",
   "metadata": {},
   "source": [
    "### Analysis 3\n",
    "*By \\<Name of person doing the analysis>*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796df2f5",
   "metadata": {},
   "source": [
    "### Analysis 4\n",
    "*By \\<Name of person doing the analysis>*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab331a",
   "metadata": {},
   "source": [
    "## Other sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ec4c9",
   "metadata": {},
   "source": [
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.**\n",
    "\n",
    "Put each model in a section of its name and mention the name of the team-member tuning the model. Below is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a185cb",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations to stakeholder(s)\n",
    "\n",
    "You may or may not have code to put in this section. Delete this section if it is irrelevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
